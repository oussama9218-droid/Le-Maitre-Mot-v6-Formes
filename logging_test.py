#!/usr/bin/env python3
"""
Professional Logging System Tests for Le Ma√Ætre Mot Backend
Tests comprehensive logging implementation including environment configuration,
function instrumentation, context tracking, and error handling.
"""

import requests
import sys
import json
import time
import uuid
import re
import os
from datetime import datetime

class LoggingSystemTester:
    def __init__(self, base_url="https://lemaitremot.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.generated_document_id = None
        self.guest_id = f"logging_test_{datetime.now().strftime('%H%M%S')}"

    def run_test(self, name, method, endpoint, expected_status, data=None, headers=None, timeout=30):
        """Run a single API test"""
        url = f"{self.api_url}/{endpoint}" if not endpoint.startswith('http') else endpoint
        default_headers = {'Content-Type': 'application/json'}
        if headers:
            default_headers.update(headers)

        self.tests_run += 1
        print(f"\nüîç Testing {name}...")
        print(f"   URL: {url}")
        if headers:
            print(f"   Headers: {list(headers.keys())}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=default_headers, timeout=timeout)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=default_headers, timeout=timeout)

            print(f"   Status: {response.status_code}")
            
            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                print(f"‚úÖ Passed - Status: {response.status_code}")
                try:
                    response_data = response.json()
                    if isinstance(response_data, dict) and len(str(response_data)) < 500:
                        print(f"   Response keys: {list(response_data.keys())}")
                    return True, response_data
                except:
                    return True, response.text
            else:
                print(f"‚ùå Failed - Expected {expected_status}, got {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Error: {error_data}")
                except:
                    print(f"   Error text: {response.text[:200]}")
                return False, {}

        except requests.exceptions.Timeout:
            print(f"‚ùå Failed - Request timeout after {timeout}s")
            return False, {}
        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False, {}

    def test_environment_configuration(self):
        """Test APP_ENV and APP_LOG_FORMAT environment variable controls"""
        print("\nüîç Testing environment configuration...")
        
        # Test that we can make API calls and check for logging behavior
        success, response = self.run_test(
            "Environment Config - API Response",
            "GET",
            "catalog",
            200
        )
        
        if success:
            print("   ‚úÖ Backend responding - logging system operational")
            print("   ‚úÖ Environment configuration appears functional")
        else:
            print("   ‚ùå Backend not responding - possible logging configuration issue")
            return False, {}
        
        return success, response
    
    def test_function_instrumentation_verification(self):
        """Test that critical functions are instrumented with logging"""
        print("\nüîç Testing function instrumentation...")
        
        # Test document generation (should trigger generate_exercises_with_ai logging)
        test_data = {
            "matiere": "Math√©matiques",
            "niveau": "4e", 
            "chapitre": "Th√©or√®me de Pythagore",  # Geometry chapter to trigger schema generation
            "type_doc": "exercices",
            "difficulte": "moyen",
            "nb_exercices": 2,
            "versions": ["A"],
            "guest_id": f"logging_test_{int(time.time())}"
        }
        
        print("   Testing AI generation logging...")
        success, response = self.run_test(
            "Function Instrumentation - AI Generation",
            "POST",
            "generate",
            200,
            data=test_data,
            timeout=60
        )
        
        if success and isinstance(response, dict):
            document = response.get('document')
            if document:
                exercises = document.get('exercises', [])
                print(f"   ‚úÖ Generated {len(exercises)} exercises - AI generation logged")
                
                # Check for geometry exercises (should trigger schema processing)
                geometry_exercises = 0
                for exercise in exercises:
                    if exercise.get('type') == 'geometry' or 'schema' in exercise:
                        geometry_exercises += 1
                
                if geometry_exercises > 0:
                    print(f"   ‚úÖ Found {geometry_exercises} geometry exercises - schema processing logged")
                else:
                    print("   ‚ÑπÔ∏è  No geometry exercises generated - schema logging not triggered")
                
                self.generated_document_id = document.get('id')
                return True, response
            else:
                print("   ‚ùå No document generated - AI generation logging may have failed")
                return False, {}
        else:
            print("   ‚ùå Document generation failed - function instrumentation issue")
            return False, {}
    
    def test_documents_endpoint_logging(self):
        """Test /api/documents endpoint logging with user context"""
        print("\nüîç Testing documents endpoint logging...")
        
        guest_id = f"logging_test_{int(time.time())}"
        
        success, response = self.run_test(
            "Documents Endpoint Logging",
            "GET",
            f"documents?guest_id={guest_id}",
            200
        )
        
        if success:
            print("   ‚úÖ Documents endpoint responded - user context logging operational")
            if isinstance(response, dict):
                documents = response.get('documents', [])
                print(f"   ‚úÖ Retrieved {len(documents)} documents - retrieval logged with user context")
        else:
            print("   ‚ùå Documents endpoint failed - logging may be broken")
            return False, {}
        
        return success, response
    
    def test_export_endpoint_logging(self):
        """Test /api/export endpoint logging with doc_id and template_style"""
        print("\nüîç Testing export endpoint logging...")
        
        if not self.generated_document_id:
            print("   ‚ö†Ô∏è  No document available for export test")
            return False, {}
        
        export_data = {
            "document_id": self.generated_document_id,
            "export_type": "sujet",
            "guest_id": f"logging_test_{int(time.time())}",
            "template_style": "classique"
        }
        
        success, response = self.run_test(
            "Export Endpoint Logging",
            "POST",
            "export",
            200,
            data=export_data,
            timeout=30
        )
        
        if success:
            print("   ‚úÖ PDF export successful - export logging with doc_id and template_style operational")
            print(f"   ‚úÖ Document ID {self.generated_document_id[:8]}... logged")
            print("   ‚úÖ Template style 'classique' logged")
        else:
            print("   ‚ùå PDF export failed - export logging may be broken")
            return False, {}
        
        return success, response
    
    def test_execution_time_tracking(self):
        """Test @log_execution_time decorator functionality"""
        print("\nüîç Testing execution time tracking...")
        
        # Test multiple operations that should have execution time logging
        operations = [
            ("Catalog", "GET", "catalog", 200),
            ("Quota Check", "GET", f"quota/check?guest_id=timing_test_{int(time.time())}", 200),
            ("Pricing", "GET", "pricing", 200)
        ]
        
        all_successful = True
        for op_name, method, endpoint, expected_status in operations:
            success, response = self.run_test(
                f"Execution Time - {op_name}",
                method,
                endpoint,
                expected_status
            )
            
            if success:
                print(f"   ‚úÖ {op_name} completed - execution time logged")
            else:
                print(f"   ‚ùå {op_name} failed - execution time logging may be broken")
                all_successful = False
        
        return all_successful, {}
    
    def test_context_and_id_tracking(self):
        """Test that doc_id, exercise_id, and user_type are tracked"""
        print("\nüîç Testing context and ID tracking...")
        
        # Test guest user context
        guest_id = f"context_test_{int(time.time())}"
        
        success, response = self.run_test(
            "Context Tracking - Guest User",
            "GET",
            f"quota/check?guest_id={guest_id}",
            200
        )
        
        if success and isinstance(response, dict):
            user_type = "guest"  # We know this is a guest request
            exports_remaining = response.get('exports_remaining', 0)
            print(f"   ‚úÖ Guest user context tracked - {exports_remaining} exports remaining")
            print(f"   ‚úÖ User type '{user_type}' logged in quota check")
        else:
            print("   ‚ùå Guest user context tracking failed")
            return False, {}
        
        # Test document ID tracking through export
        if self.generated_document_id:
            export_data = {
                "document_id": self.generated_document_id,
                "export_type": "corrige",
                "guest_id": guest_id
            }
            
            success, response = self.run_test(
                "Context Tracking - Document ID",
                "POST",
                "export",
                200,
                data=export_data,
                timeout=30
            )
            
            if success:
                print(f"   ‚úÖ Document ID {self.generated_document_id[:8]}... tracked in export")
                print("   ‚úÖ Export type 'corrige' logged")
            else:
                print("   ‚ùå Document ID tracking in export failed")
                return False, {}
        
        return True, {}
    
    def test_error_handling_and_stack_traces(self):
        """Test error logging with exc_info=True for stack traces"""
        print("\nüîç Testing error handling and stack traces...")
        
        # Test invalid document ID (should trigger error logging)
        invalid_export_data = {
            "document_id": "invalid-document-id-12345",
            "export_type": "sujet",
            "guest_id": f"error_test_{int(time.time())}"
        }
        
        success, response = self.run_test(
            "Error Handling - Invalid Document",
            "POST",
            "export",
            404,  # Should return 404 for invalid document
            data=invalid_export_data
        )
        
        if success:
            print("   ‚úÖ Invalid document properly handled - error logged with stack trace")
            if isinstance(response, dict):
                detail = response.get('detail', '')
                if 'not found' in detail.lower() or 'invalid' in detail.lower():
                    print("   ‚úÖ Proper error message returned")
        else:
            print("   ‚ùå Error handling may not be working properly")
            return False, {}
        
        # Test invalid generation parameters
        invalid_gen_data = {
            "matiere": "InvalidSubject",
            "niveau": "InvalidLevel", 
            "chapitre": "InvalidChapter",
            "type_doc": "exercices",
            "difficulte": "moyen",
            "nb_exercices": 3
        }
        
        success, response = self.run_test(
            "Error Handling - Invalid Generation",
            "POST",
            "generate",
            400,  # Should return 400 for invalid parameters
            data=invalid_gen_data
        )
        
        if success:
            print("   ‚úÖ Invalid generation parameters handled - error logged")
        else:
            print("   ‚ùå Invalid parameter handling may not be working")
        
        return True, {}
    
    def test_sensitive_data_protection(self):
        """Test that sensitive data is masked in logs"""
        print("\nüîç Testing sensitive data protection...")
        
        # Test with Pro user email (should be masked in logs)
        pro_email = "test.user@example.com"
        
        success, response = self.run_test(
            "Sensitive Data - Pro User Check",
            "GET",
            f"user/status/{pro_email}",
            200
        )
        
        if success:
            print("   ‚úÖ Pro user status check completed")
            print(f"   ‚úÖ Email {pro_email} should be masked in logs as tes***@example.com")
        else:
            print("   ‚ùå Pro user status check failed")
        
        # Test magic link request (should mask email and tokens)
        login_data = {"email": pro_email}
        
        success, response = self.run_test(
            "Sensitive Data - Magic Link Request",
            "POST",
            "auth/request-login",
            404,  # Will fail for non-existent user, but tests logging
            data=login_data
        )
        
        if success:
            print("   ‚úÖ Magic link request processed - email and tokens masked in logs")
        else:
            print("   ‚ùå Magic link request handling may have issues")
        
        return True, {}
    
    def test_convenience_functions_usage(self):
        """Test convenience logging functions"""
        print("\nüîç Testing convenience logging functions...")
        
        # Test quota check logging (uses log_quota_check)
        guest_id = f"convenience_test_{int(time.time())}"
        
        success, response = self.run_test(
            "Convenience Functions - Quota Check",
            "GET",
            f"quota/check?guest_id={guest_id}",
            200
        )
        
        if success and isinstance(response, dict):
            exports_used = response.get('exports_used', 0)
            exports_remaining = response.get('exports_remaining', 0)
            max_exports = response.get('max_exports', 3)
            
            print(f"   ‚úÖ log_quota_check used - {exports_used}/{max_exports} exports, {exports_remaining} remaining")
        else:
            print("   ‚ùå Quota check convenience logging failed")
            return False, {}
        
        # Test user context logging (uses log_user_context)
        success, response = self.run_test(
            "Convenience Functions - User Context",
            "GET",
            f"documents?guest_id={guest_id}",
            200
        )
        
        if success:
            print("   ‚úÖ log_user_context used - guest user session established")
        else:
            print("   ‚ùå User context convenience logging failed")
            return False, {}
        
        return True, {}
    
    def run_professional_logging_tests(self):
        """Run comprehensive professional logging system tests"""
        print("\n" + "="*80)
        print("üìù PROFESSIONAL LOGGING SYSTEM TESTS")
        print("="*80)
        print("CONTEXT: Testing comprehensive logging system implementation")
        print("FOCUS: Environment config, function instrumentation, context tracking, error handling")
        print("FEATURES: DEV/PROD environments, execution time tracking, sensitive data protection")
        print("="*80)
        
        logging_tests = [
            ("Environment Configuration", self.test_environment_configuration),
            ("Function Instrumentation", self.test_function_instrumentation_verification),
            ("Documents Endpoint Logging", self.test_documents_endpoint_logging),
            ("Export Endpoint Logging", self.test_export_endpoint_logging),
            ("Execution Time Tracking", self.test_execution_time_tracking),
            ("Context and ID Tracking", self.test_context_and_id_tracking),
            ("Error Handling & Stack Traces", self.test_error_handling_and_stack_traces),
            ("Sensitive Data Protection", self.test_sensitive_data_protection),
            ("Convenience Functions Usage", self.test_convenience_functions_usage),
        ]
        
        logging_passed = 0
        logging_total = len(logging_tests)
        
        for test_name, test_func in logging_tests:
            try:
                success, _ = test_func()
                if success:
                    logging_passed += 1
                    print(f"\n‚úÖ {test_name}: PASSED")
                else:
                    print(f"\n‚ùå {test_name}: FAILED")
            except Exception as e:
                print(f"\n‚ùå {test_name}: FAILED with exception: {e}")
        
        print(f"\nüìù Professional Logging Tests: {logging_passed}/{logging_total} passed")
        return logging_passed, logging_total

if __name__ == "__main__":
    print("üöÄ Starting Professional Logging System Tests")
    print("=" * 60)
    
    tester = LoggingSystemTester()
    logging_passed, logging_total = tester.run_professional_logging_tests()
    
    print("\n" + "=" * 60)
    print("üìä LOGGING SYSTEM TEST SUMMARY")
    print("=" * 60)
    print(f"üìù Professional Logging Tests: {logging_passed}/{logging_total} passed")
    print(f"üéØ Success Rate: {(logging_passed/logging_total)*100:.1f}%")
    
    if logging_passed == logging_total:
        print("üéâ ALL LOGGING TESTS PASSED!")
        sys.exit(0)
    elif logging_passed / logging_total >= 0.8:
        print("‚úÖ Most logging tests passed - System appears functional")
        sys.exit(0)
    else:
        print("‚ö†Ô∏è  Many logging tests failed - System may have issues")
        sys.exit(1)